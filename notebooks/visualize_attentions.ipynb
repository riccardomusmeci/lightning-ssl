{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from src.io.io import load_config\n",
    "from src.dataset import SSLDataset\n",
    "from src.transform import Transform\n",
    "from torch.utils.data import DataLoader\n",
    "from src.model.vit import compute_attentions\n",
    "from src.model.utils import create_model, load_state_dict_ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checkpoints + Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"model/model.ckpt\"\n",
    "config_path = \"model/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(path=config_path)\n",
    "config[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = config[\"model\"][\"backbone\"]\n",
    "IMG_SIZE = config[\"transform\"][\"img_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "    backbone=BACKBONE,\n",
    "    pretrained=False,\n",
    "    img_size=IMG_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_state_dict_ssl(\n",
    "    model=model,\n",
    "    ssl_state_dict=torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up SSLTransform + ODIN Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    framework=\"dino\",\n",
    "    train=False,\n",
    "    img_size=IMG_SIZE\n",
    ")\n",
    "dataset = SSLDataset(\n",
    "    root_dir=\"images\",\n",
    "    split=\"val\",\n",
    "    with_folders=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(a=0, b=len(dataset)-1)\n",
    "img_path = dataset.img_paths[i]\n",
    "    \n",
    "img = Image.open(img_path)\n",
    "img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# Augmentation\n",
    "x, views = transform(img=img)\n",
    "\n",
    "# Input Tensor\n",
    "x = torch.from_numpy(x).unsqueeze(dim=0)\n",
    "print(f\"dataset index {i} - path: {dataset.img_paths[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Attentions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = compute_attentions(\n",
    "    model=model,\n",
    "    x=x, \n",
    "    patch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing images to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img = np.array(img)\n",
    "mask = np.sum(attentions, axis=0)\n",
    "mask = cv2.blur(mask,(10,10))\n",
    "mask = np.stack([mask, mask, mask], axis=-1)\n",
    "mask = mask / mask.max()\n",
    "result = (mask * img).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5), nrows=1, ncols=3)\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title(f\"Original\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(mask)\n",
    "ax[1].set_title(\"Attention mask\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "ax[2].imshow(result)\n",
    "ax[2].set_title(f\"{BACKBONE} - Attention on image\")\n",
    "ax[2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2c7cd666a42c999a129ff3dbda78336d983b42f613c7c18cacdcabe088cd477"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
